{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing various NN's on *multivariate* time series**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-BEATS\n",
    "from neuralforecast.models import NBEATS, PatchTST, NBEATSx\n",
    "from neuralforecast.losses.pytorch import HuberLoss\n",
    "from neuralforecast.core import NeuralForecast\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# N-HiTS\n",
    "from darts import TimeSeries\n",
    "from darts.models import NHiTSModel\n",
    "from torch.nn import MSELoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaling(x):\n",
    "    mean = np.mean(np.abs(x))\n",
    "    s = np.std(x)\n",
    "    if s == 0:\n",
    "        return x    \n",
    "    return (x - mean)/s\n",
    "\n",
    "def standard_unscaling(original, scaled):\n",
    "    mean = np.mean(np.abs(original))\n",
    "    s = np.std(original)\n",
    "\n",
    "    return (scaled * s) + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1** N-BEATSx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1** Ground water data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "aquifer_by_stations = joblib.load('../data/interim/ground-water-and-weather-no-new-features.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add the predictions\n",
    "def weather_forecast(X):\n",
    "    postfixes = ['avg', 'min', 'max']\n",
    "    for postfix in postfixes:\n",
    "        X[f\"precipitation_probability_{postfix}_shift-{5}\"] = X[f'precipitation_probability_{postfix}'].shift(-5)\n",
    "        X[f\"precipitation_intensity_{postfix}_shift-{5}\"] = X[f'precipitation_intensity_{postfix}'].shift(-5)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquifers_list = [85065, 85064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weather forecast features\n",
    "for aquifer in aquifers_list:\n",
    "    aquifer_by_stations[aquifer] = weather_forecast(aquifer_by_stations[aquifer])\n",
    "    aquifer_by_stations[aquifer] = aquifer_by_stations[aquifer][:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional parameters that will be used for training and prediction\n",
    "additional_parameters = aquifer_by_stations[85065].columns.values.tolist()\n",
    "\n",
    "# Clean up the list\n",
    "parameters_to_delete = ['date', 'station_id', 'altitude', 'altitude_diff', 'id', 'location_id']\n",
    "additional_parameters = [parameter for parameter in additional_parameters if parameter not in parameters_to_delete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the horizon and the day_len\n",
    "horizon = 5\n",
    "day_len = 100\n",
    "test_len = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquifers_list = [85065, 85064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the additional features\n",
    "for aquifer in aquifers_list:\n",
    "    for feature in additional_parameters:\n",
    "        aquifer_by_stations[aquifer][feature] = standard_scaling(aquifer_by_stations[aquifer][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which contains parameters to tune and the model\n",
    "\n",
    "def objective(trial):\n",
    "    input_size = trial.suggest_int('input_size', 5, 20)\n",
    "    \n",
    "    n_harmonics = trial.suggest_int('n_harmonics', 1, 5)\n",
    "    n_polynomials = trial.suggest_int('n_polynomials', 1, 5)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1)\n",
    "\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 600)\n",
    "\n",
    "    validation_size = trial.suggest_int('val_size', 5, 15)\n",
    "\n",
    "    scaling = trial.suggest_categorical('scaling', [True, False])\n",
    "\n",
    "    \n",
    "\n",
    "    models = [NBEATSx(h=horizon,input_size=input_size,\n",
    "                 max_steps=max_steps,\n",
    "                 learning_rate=learning_rate,\n",
    "                 n_harmonics=n_harmonics,\n",
    "                 n_polynomials=n_polynomials,\n",
    "                 hist_exog_list=additional_parameters,\n",
    "                 accelerator='cuda',\n",
    "                 logger=False)\n",
    "                 ]\n",
    "    model = NeuralForecast(models=models, freq='D')\n",
    "\n",
    "    # List for r2 results for different prediction horizons\n",
    "    r2_scores = [[] for _ in range(horizon)]\n",
    "    \n",
    "    for aquifer in aquifers_list:\n",
    "        # List for storing the predictions\n",
    "        predictions = [[] for _ in range(5)]\n",
    "\n",
    "        # Get the dataset for the aquifer\n",
    "        y = aquifer_by_stations[aquifer][:-test_len]\n",
    "\n",
    "        # Rename the columns (library wants to have specific names)\n",
    "        y = y.rename(columns={'date':'ds', 'altitude_diff':'y', 'station_id':'unique_id'})\n",
    "\n",
    "        # Scaling\n",
    "        if scaling:\n",
    "            y['y'] = standard_scaling(y['y'])\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(y[:-day_len], val_size=validation_size)\n",
    "\n",
    "        # Iterate from day_len days before the end, to the last day\n",
    "        for i in range(day_len + (horizon-1), 0, -1):\n",
    "            \n",
    "            # Predict\n",
    "            forecast = model.predict(df=y[:-i], verbose=0)\n",
    "\n",
    "            # Unscale\n",
    "            if scaling:\n",
    "                forecast['NBEATSx'] = standard_unscaling(aquifer_by_stations[aquifer]['altitude_diff'], forecast['NBEATSx'])\n",
    "                \n",
    "\n",
    "            # Store the results for every prediction horizon separately\n",
    "            for i in range(horizon):\n",
    "                predictions[i].append(forecast['NBEATSx'].values[i])\n",
    "        \n",
    "        # Clean up the results\n",
    "        predictions[0] = predictions[0][-day_len:]\n",
    "        predictions[1] = predictions[1][3:-1]\n",
    "        predictions[2] = predictions[2][2:-2]\n",
    "        predictions[3] = predictions[3][1:-3]\n",
    "        predictions[4] = predictions[4][0:-4]\n",
    "\n",
    "        # Calculate the r2 scores and store them in a list\n",
    "        for i in range(horizon):\n",
    "            r2_scores[i].append(r2_score(y['y'][-day_len:], predictions[i]))\n",
    "    \n",
    "    # Calculate the average r2 score\n",
    "    r2_average =  []\n",
    "    \n",
    "    for i in range(5):\n",
    "        r2_average.append(np.mean(r2_scores[i]))\n",
    "\n",
    "    # Set the loss as average of average r2 scores for different prediction horizons\n",
    "    loss = np.mean(r2_average)\n",
    "\n",
    "    print(r2_average)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on multiple stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquifers_list = [85065, 85064]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><sup>!!! The scaling is mandatory, otherwise the results are very bad</sup></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the additional features\n",
    "for aquifer in aquifers_list:\n",
    "    for feature in additional_parameters:\n",
    "        aquifer_by_stations[aquifer][feature] = standard_scaling(aquifer_by_stations[aquifer][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''{'input_size': 7,\n",
    " 'n_harmonics': 2,\n",
    " 'n_polynomials': 3,\n",
    " 'learning_rate': 0.009866118273076567,\n",
    " 'max_steps': 520,\n",
    " 'val_size': 9,\n",
    " 'scaling': True}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 5 # prediction horizon\n",
    "day_len = 365 # number of days to forecast\n",
    "val_size = 9\n",
    "\n",
    "models = [NBEATSx(h=horizon, \n",
    "                 accelerator='cuda',\n",
    "                 input_size=15,\n",
    "                 #n_harmonics=2,\n",
    "                 #n_polynomials=3,\n",
    "                 #learning_rate=0.01,\n",
    "                 max_steps=500,\n",
    "                 hist_exog_list=additional_parameters,\n",
    "                 devices=[0],\n",
    "                 logger=False)]\n",
    "\n",
    "model = NeuralForecast(models=models, freq='D')\n",
    "\n",
    "# List for r2 results for different prediction horizons\n",
    "r2_scores = [[] for _ in range(horizon)]\n",
    "\n",
    "for aquifer in aquifers_list:\n",
    "    # List for storing the predictions\n",
    "    predictions = [[] for _ in range(5)]\n",
    "\n",
    "    # Get the dataset for the aquifer\n",
    "    y = aquifer_by_stations[aquifer]\n",
    "\n",
    "    # Rename the columns (library wants to have specific names)\n",
    "    y = y.rename(columns={'date':'ds', 'altitude_diff':'y', 'station_id':'unique_id'})\n",
    "\n",
    "    # Scaling\n",
    "    y['y'] = standard_scaling(y['y'])\n",
    "\n",
    "    # Only keep these 3 columns\n",
    "    #y = y[['ds', 'y', 'unique_id']]\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(y[:-day_len], val_size=val_size)\n",
    "\n",
    "    # Iterate from day_len days before the end, to the last day\n",
    "    for i in range(day_len + (horizon-1), 0, -1):\n",
    "        \n",
    "        # Predict\n",
    "        forecast = model.predict(df=y[:-i], verbose=0)\n",
    "\n",
    "        # Unscale\n",
    "        forecast['NBEATSx'] = standard_unscaling(aquifer_by_stations[aquifer]['altitude_diff'], forecast['NBEATSx'])\n",
    "\n",
    "        # Store the results for every prediction horizon separately\n",
    "        for i in range(horizon):\n",
    "            predictions[i].append(forecast['NBEATSx'].values[i])\n",
    "    \n",
    "    # Clean up the results\n",
    "    predictions[0] = predictions[0][-day_len:]\n",
    "    predictions[1] = predictions[1][3:-1]\n",
    "    predictions[2] = predictions[2][2:-2]\n",
    "    predictions[3] = predictions[3][1:-3]\n",
    "    predictions[4] = predictions[4][0:-4]\n",
    "\n",
    "    # Calculate the r2 scores and store them in a list\n",
    "    for i in range(horizon):\n",
    "        r2_scores[i].append(r2_score(aquifer_by_stations[aquifer]['altitude_diff'][-day_len:], predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average r2 score\n",
    "r2_average =  []\n",
    "std_dev = []\n",
    "\n",
    "for i in range(5):\n",
    "    r2_average.append(np.mean(r2_scores[i]))\n",
    "    std_dev.append(np.std(r2_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(aquifer_by_stations[aquifer]['date'][-350:-250], aquifer_by_stations[aquifer]['altitude_diff'][-350:-250], color=\"royalblue\", label=\"true data\")\n",
    "#plt.plot(aquifer_by_stations[aquifer]['date'][-200:], aquifer_by_stations[aquifer]['precipitation'][-200:].apply(lambda x: x/50), color=\"olive\", label=\"true data\")\n",
    "#plt.plot(aquifer_by_stations[aquifer]['date'][-200:], predictions[0][-200:], color=\"tomato\", label=\"forecast\")\n",
    "#plt.plot(aquifer_by_stations[aquifer]['date'][-200:], predictions[1][-200:], color=\"green\", label=\"forecast\")\n",
    "plt.plot(aquifer_by_stations[aquifer]['date'][-350:-250], predictions[4][-350:-250], color=\"goldenrod\", label=\"forecast\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the average r2_scores\n",
    "with open('../reports/n-beatsx/n-beatxs-ground-water-r2.txt', 'w') as file:\n",
    "    for item in r2_average:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the standard deviations\n",
    "with open('../reports/n-beatsx/n-beatsx-ground-water-std-dev.txt', 'w') as file:\n",
    "    for item in std_dev:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the r2_scores list\n",
    "r2_scores_transposed = [list(x) for x in zip(*r2_scores)]\n",
    "# Pair up the stations with their r2_scores and store them in a dictionary\n",
    "scores = dict(zip(aquifers_list, r2_scores_transposed))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort them by the value in r2_scores[0]\n",
    "scores_sorted = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1][0])}\n",
    "scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the r2_scores\n",
    "joblib.dump(scores_sorted, '../reports/n-beatsx/n-beatsx-ground-water-r2-stations.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2** Surface water data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "watercourse_by_stations = joblib.load('../data/interim/surface-water-and-weather-no-new-features.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of station used for testing\n",
    "station_list = [2530, 2620, 4200, 4230, 4270, 4515, 4520, 4570, 4575, 5040, 5078, 5330, 5425, 5500, 6060, 6068, 6200, 6220, 6300, 6340, 8454, 8565]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weather forecast features\n",
    "for station in station_list:\n",
    "    watercourse_by_stations[station] = weather_forecast(watercourse_by_stations[station])\n",
    "    watercourse_by_stations[station] = watercourse_by_stations[station][:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional parameters that will be used for training and prediction\n",
    "additional_parameters = watercourse_by_stations[2530].columns.values.tolist()\n",
    "\n",
    "# Clean up the list\n",
    "parameters_to_delete = ['date', 'station_id', 'level', 'level_diff', 'id', 'location_id']\n",
    "additional_parameters = [parameter for parameter in additional_parameters if parameter not in parameters_to_delete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparemeter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the horizon and the day_len\n",
    "horizon = 5\n",
    "day_len = 100\n",
    "test_len = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = [4270, 4570, 4515, 6068]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the additional features\n",
    "for station in station_list:\n",
    "    for feature in additional_parameters:\n",
    "        watercourse_by_stations[station][feature] = standard_scaling(watercourse_by_stations[station][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which contains parameters to tune the model\n",
    "\n",
    "def objective(trial):\n",
    "    input_size = trial.suggest_int('input_size', 5, 20)\n",
    "    \n",
    "    n_harmonics = trial.suggest_int('n_harmonics', 1, 5)\n",
    "    n_polynomials = trial.suggest_int('n_polynomials', 1, 5)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1)\n",
    "\n",
    "    max_steps = trial.suggest_int('max_steps', 10, 1000)\n",
    "\n",
    "    validation_size = trial.suggest_int('val_size', 5, 15)\n",
    "\n",
    "    models = [NBEATSx(h=horizon,input_size=input_size,\n",
    "                 max_steps=max_steps,\n",
    "                 learning_rate=learning_rate,\n",
    "                 n_harmonics=n_harmonics,\n",
    "                 n_polynomials=n_polynomials,\n",
    "                 hist_exog_list=additional_parameters,\n",
    "                 accelerator='cuda',\n",
    "                 logger=False)\n",
    "                 ]\n",
    "    model = NeuralForecast(models=models, freq='D')\n",
    "\n",
    "    # List for r2 results for different prediction horizons\n",
    "    r2_scores = [[] for _ in range(horizon)]\n",
    "    \n",
    "    for station in station_list:\n",
    "        # List for storing the predictions\n",
    "        predictions = [[] for _ in range(5)]\n",
    "\n",
    "        # Get the dataset for the aquifer\n",
    "        y = watercourse_by_stations[station][:-test_len]\n",
    "\n",
    "        # Rename the columns (library wants to have specific names)\n",
    "        y = y.rename(columns={'date':'ds', 'level_diff':'y', 'station_id':'unique_id'})\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(y[:-day_len], val_size=validation_size)\n",
    "\n",
    "        # Iterate from day_len days before the end, to the last day\n",
    "        for i in range(day_len + (horizon-1), 0, -1):\n",
    "            \n",
    "            # Predict\n",
    "            forecast = model.predict(df=y[:-i], verbose=0)\n",
    "\n",
    "            # Store the results for every prediction horizon separately\n",
    "            for i in range(horizon):\n",
    "                predictions[i].append(forecast['NBEATS'].values[i])\n",
    "        \n",
    "        # Clean up the results\n",
    "        predictions[0] = predictions[0][-day_len:]\n",
    "        predictions[1] = predictions[1][3:-1]\n",
    "        predictions[2] = predictions[2][2:-2]\n",
    "        predictions[3] = predictions[3][1:-3]\n",
    "        predictions[4] = predictions[4][0:-4]\n",
    "\n",
    "        # Calculate the r2 scores and store them in a list\n",
    "        for i in range(horizon):\n",
    "            r2_scores[i].append(r2_score(y['y'][-day_len:], predictions[i]))\n",
    "    \n",
    "    # Calculate the average r2 score\n",
    "    r2_average =  []\n",
    "    \n",
    "    for i in range(5):\n",
    "        r2_average.append(np.mean(r2_scores[i]))\n",
    "\n",
    "    # Set the loss as average of average r2 scores for different prediction horizons\n",
    "    loss = np.mean(r2_average)\n",
    "\n",
    "    print(r2_average)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on multiple stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of station used for testing\n",
    "station_list = [2530, 2620, 4200, 4230, 4270, 4515, 4520, 4570, 4575, 5040, 5078, 5330, 5425, 5500, 6060, 6068, 6200, 6220, 6300, 6340, 8454, 8565]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = [2530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the additional features\n",
    "for station in station_list:\n",
    "    for feature in additional_parameters:\n",
    "        watercourse_by_stations[station][feature] = standard_scaling(watercourse_by_stations[station][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 5 # prediction horizon\n",
    "day_len = 365 # number of days to forecast\n",
    "val_size = 2*horizon\n",
    "\n",
    "models = [NBEATSx(h=horizon, \n",
    "                 #loss=HuberLoss(),\n",
    "                 accelerator='cuda',\n",
    "                 input_size=2*horizon,\n",
    "                 #n_harmonics=2,\n",
    "                 #n_polynomials=4,\n",
    "                 #scaler_type='robust',\n",
    "                 #learning_rate=0.01,\n",
    "                 max_steps=500,\n",
    "                 #n_blocks=[1, 1, 3],\n",
    "                 #mlp_units=[[32, 32], [32, 32]],\n",
    "                 hist_exog_list=additional_parameters,\n",
    "                 logger=False)]\n",
    "\n",
    "model = NeuralForecast(models=models, freq='D')\n",
    "\n",
    "# List for r2 results for different prediction horizons\n",
    "r2_scores = [[] for _ in range(horizon)]\n",
    "\n",
    "for station in station_list:\n",
    "    # List for storing the predictions\n",
    "    predictions = [[] for _ in range(5)]\n",
    "\n",
    "    # Get the dataset for the aquifer\n",
    "    y = watercourse_by_stations[station]\n",
    "\n",
    "    # Rename the columns (library wants to have specific names)\n",
    "    y = y.rename(columns={'date':'ds', 'level_diff':'y', 'station_id':'unique_id'})\n",
    "\n",
    "    # Scale the level_diff\n",
    "    #y['y'] = standard_scaling(y['y'])\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(y[:-day_len], val_size=horizon)\n",
    "\n",
    "    # Iterate from day_len days before the end, to the last day\n",
    "    for i in range(day_len + (horizon-1), 0, -1):\n",
    "        \n",
    "        # Predict\n",
    "        forecast = model.predict(df=y[:-i], verbose=0)\n",
    "\n",
    "        # Unscale the predictions\n",
    "        #forecast['NBEATSx'] = standard_unscaling(watercourse_by_stations[station]['level_diff'], forecast['NBEATSx'])\n",
    "\n",
    "        # Store the results for every prediction horizon separately\n",
    "        for i in range(horizon):\n",
    "            predictions[i].append(forecast['NBEATSx'].values[i])\n",
    "    \n",
    "    # Clean up the results\n",
    "    predictions[0] = predictions[0][-day_len:]\n",
    "    predictions[1] = predictions[1][3:-1]\n",
    "    predictions[2] = predictions[2][2:-2]\n",
    "    predictions[3] = predictions[3][1:-3]\n",
    "    predictions[4] = predictions[4][0:-4]\n",
    "\n",
    "    # Calculate the r2 scores and store them in a list\n",
    "    for i in range(horizon):\n",
    "        r2_scores[i].append(r2_score(watercourse_by_stations[station]['level_diff'][-day_len:], predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average r2 score\n",
    "r2_average =  []\n",
    "std_dev = []\n",
    "\n",
    "for i in range(5):\n",
    "    r2_average.append(np.mean(r2_scores[i]))\n",
    "    std_dev.append(np.std(r2_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(watercourse_by_stations[station]['date'][-200:], watercourse_by_stations[station]['level_diff'][-200:], color=\"royalblue\", label=\"true data\")\n",
    "plt.plot(watercourse_by_stations[station]['date'][-200:], predictions[0][-200:], color=\"tomato\", label=\"forecast\")\n",
    "plt.plot(watercourse_by_stations[station]['date'][-200:], predictions[4][-200:], color=\"goldenrod\", label=\"forecast\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the average r2_scores\n",
    "with open('../reports/n-beatsx/n-beatsx-surface-water-r2.txt', 'w') as file:\n",
    "    for item in r2_average:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the standard deviations\n",
    "with open('../reports/n-beatsx/n-beatsx-surface-water-std-dev.txt', 'w') as file:\n",
    "    for item in std_dev:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the r2_scores list\n",
    "r2_scores_transposed = [list(x) for x in zip(*r2_scores)]\n",
    "# Pair up the stations with their r2_scores and store them in a dictionary\n",
    "scores = dict(zip(station_list, r2_scores_transposed))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort them by the value in r2_scores[0]\n",
    "scores_sorted = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1][0])}\n",
    "scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the r2_scores\n",
    "joblib.dump(scores_sorted, '../reports/n-beatsx/n-beatsx-surface-water-r2-stations.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2** N-HiTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1** Ground water data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "aquifer_by_stations = joblib.load('../data/interim/ground-water-and-weather-no-new-features.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquifers_list = [85065, 85064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weather forecast features\n",
    "for aquifer in aquifers_list:\n",
    "    aquifer_by_stations[aquifer] = weather_forecast(aquifer_by_stations[aquifer])\n",
    "    aquifer_by_stations[aquifer] = aquifer_by_stations[aquifer][:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The additional parameters that will be used for training and prediction\n",
    "additional_parameters = aquifer_by_stations[85065].columns.values.tolist()\n",
    "\n",
    "# Clean up the list\n",
    "parameters_to_delete = ['date', 'station_id', 'altitude', 'altitude_diff', 'id', 'location_id']\n",
    "additional_parameters = [parameter for parameter in additional_parameters if parameter not in parameters_to_delete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the horizon, day_len (number of predicted days), test_len (number of days used for final testing)\n",
    "horizon = 5\n",
    "day_len = 100\n",
    "test_len = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations to test\n",
    "aquifers_list = [85065, 85064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function which contains parameters to tune and the model\n",
    "\n",
    "def objective(trial):\n",
    "    input_chunk_length = trial.suggest_int('input_chunk_length', 5, 70)\n",
    "    output_chunk_length = trial.suggest_int('output_chunk_length', 1, 10)\n",
    "    num_stacks = trial.suggest_int('num_stacks', 1, 4)\n",
    "    num_blocks = trial.suggest_int('num_blocks', 1, 3)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 5)\n",
    "    layer_widths = trial.suggest_categorical('layer_widths', [64, 128, 256, 512])\n",
    "    dropout = trial.suggest_categorical('dropout', [0.1, 0.2])\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "    n_epochs = trial.suggest_int('n_epochs', 10, 200)\n",
    "\n",
    "    model = NHiTSModel(input_chunk_length=input_chunk_length,\n",
    "                     output_chunk_length=output_chunk_length,\n",
    "                     num_stacks=num_stacks,\n",
    "                     num_blocks=num_blocks,\n",
    "                     num_layers=num_layers,\n",
    "                     layer_widths=layer_widths,\n",
    "                     dropout=dropout,\n",
    "                     optimizer_kwargs={'lr': learning_rate},\n",
    "                     n_epochs=n_epochs,\n",
    "                     pl_trainer_kwargs={'logger': False, \"accelerator\": \"gpu\", \"devices\": [0]})\n",
    "    \n",
    "\n",
    "\n",
    "    # List for r2 results for different prediction horizons\n",
    "    r2_scores = [[] for _ in range(horizon)]\n",
    "    \n",
    "    for aquifer in aquifers_list:\n",
    "        # List for storing the predictions\n",
    "        predictions = [[] for _ in range(5)]\n",
    "\n",
    "        # Get the dataset for the aquifer\n",
    "        y = aquifer_by_stations[aquifer][:-test_len]\n",
    "\n",
    "        # Change to TimeSeries format (required by the library)\n",
    "        y = TimeSeries.from_dataframe(y, time_col='date', value_cols='altitude_diff')\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(y[:-day_len])\n",
    "\n",
    "        # Iterate from day_len days before the end, to the last day\n",
    "        for i in range(day_len + (horizon-1), 0, -1):\n",
    "            \n",
    "            # Predict\n",
    "            forecast = model.predict(n=horizon, series=y[:-i])\n",
    "\n",
    "\n",
    "            # Store the results for every prediction horizon separately\n",
    "            for i in range(horizon):\n",
    "                predictions[i].append(forecast.values()[i][0])\n",
    "        \n",
    "        # Clean up the results\n",
    "        predictions[0] = predictions[0][-day_len:]\n",
    "        predictions[1] = predictions[1][3:-1]\n",
    "        predictions[2] = predictions[2][2:-2]\n",
    "        predictions[3] = predictions[3][1:-3]\n",
    "        predictions[4] = predictions[4][0:-4]\n",
    "\n",
    "        # Calculate the r2 scores and store them in a list\n",
    "        for i in range(horizon):\n",
    "            r2_scores[i].append(r2_score(aquifer_by_stations[aquifer]['altitude_diff'][-(day_len+test_len):-test_len], predictions[i]))\n",
    "    \n",
    "    # Calculate the average r2 score\n",
    "    r2_average =  []\n",
    "    \n",
    "    for i in range(5):\n",
    "        r2_average.append(np.mean(r2_scores[i]))\n",
    "\n",
    "    # Set the loss as average of average r2 scores for different prediction horizons\n",
    "    loss = np.mean(r2_average)\n",
    "\n",
    "    print(r2_average)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test on multiple stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquifers_list = [85065, 85064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the additional features\n",
    "for aquifer in aquifers_list:\n",
    "    for feature in additional_parameters:\n",
    "        aquifer_by_stations[aquifer][feature] = standard_scaling(aquifer_by_stations[aquifer][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquifers_list = [85065]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''{'input_chunk_length': 46,\n",
    " 'output_chunk_length': 8,\n",
    " 'num_stacks': 3,\n",
    " 'num_blocks': 2,\n",
    " 'num_layers': 4,\n",
    " 'layer_widths': 512,\n",
    " 'dropout': 0.2,\n",
    " 'learning_rate': 0.0001,\n",
    " 'n_epochs': 89}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 5 # prediction horizon\n",
    "day_len = 365 # number of days to forecast\n",
    "\n",
    "# Set the model parameters\n",
    "model = NHiTSModel(\n",
    "    input_chunk_length=46,\n",
    "    output_chunk_length=8,\n",
    "    #num_blocks=2,\n",
    "    #num_stacks=3,\n",
    "    #num_layers=4,\n",
    "    #layer_widths=512,\n",
    "    #dropout=0.2,\n",
    "    n_epochs=20,\n",
    "    optimizer_kwargs={'lr': 1e-4},\n",
    "    pl_trainer_kwargs={'logger': False, \"accelerator\": \"gpu\", \"devices\": -1}\n",
    ")\n",
    "\n",
    "\n",
    "# List for r2 results for different prediction horizons\n",
    "r2_scores = [[] for _ in range(horizon)]\n",
    "\n",
    "for aquifer in aquifers_list:\n",
    "    # List for storing the predictions\n",
    "    predictions = [[] for _ in range(5)]\n",
    "\n",
    "    # Get the dataset for the aquifer\n",
    "    y = aquifer_by_stations[aquifer].copy()\n",
    "\n",
    "    # Scale the target values\n",
    "    #y['altitude_diff'] = standard_scaling(y['altitude_diff'])\n",
    "\n",
    "    # Change the format to TimeSeries\n",
    "    y = TimeSeries.from_dataframe(y, time_col='date', value_cols='altitude_diff')\n",
    "    \n",
    "    # Change the additional features to TimeSeries\n",
    "    additional_parameters_TS  = TimeSeries.from_dataframe(aquifer_by_stations[aquifer], time_col='date', \n",
    "                                                          value_cols=additional_parameters)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(y[:-day_len], past_covariates=additional_parameters_TS[:-day_len])\n",
    "\n",
    "    # Iterate from day_len days before the end, to the last day\n",
    "    for i in range(day_len + (horizon-1), 0, -1):\n",
    "        \n",
    "        # Make predictions\n",
    "        forecast = model.predict(n=horizon, series=y[:-i], past_covariates=additional_parameters_TS[:-i])\n",
    "\n",
    "        # Unscale the predictions\n",
    "        #for i in range(horizon):\n",
    "            #forecast.values()[i][0] = standard_unscaling(aquifer_by_stations[aquifer]['altitude_diff'], forecast.values()[i][0])\n",
    "\n",
    "        # Store the results for every prediction horizon separately\n",
    "        for i in range(horizon):\n",
    "            predictions[i].append(forecast.values()[i][0])\n",
    "    \n",
    "    # Clean up the results\n",
    "    predictions[0] = predictions[0][-day_len:]\n",
    "    predictions[1] = predictions[1][3:-1]\n",
    "    predictions[2] = predictions[2][2:-2]\n",
    "    predictions[3] = predictions[3][1:-3]\n",
    "    predictions[4] = predictions[4][0:-4]\n",
    "\n",
    "    # Calculate the r2 scores and store them in a list\n",
    "    for i in range(horizon):\n",
    "        r2_scores[i].append(r2_score(aquifer_by_stations[aquifer]['altitude_diff'][-day_len:], predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average r2 score\n",
    "r2_average =  []\n",
    "std_dev = []\n",
    "\n",
    "for i in range(5):\n",
    "    r2_average.append(np.mean(r2_scores[i]))\n",
    "    std_dev.append(np.std(r2_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(aquifer_by_stations[aquifer]['date'][-200:], aquifer_by_stations[aquifer]['altitude_diff'][-200:], color=\"royalblue\", label=\"true data\")\n",
    "plt.plot(aquifer_by_stations[aquifer]['date'][-200:], predictions[0][-200:], color=\"tomato\", label=\"forecast\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the average r2_scores\n",
    "with open('../reports/n-hits/n-hits-ground-water-r2.txt', 'w') as file:\n",
    "    for item in r2_average:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the standard deviations\n",
    "with open('../reports/n-hits/n-hits-ground-water-std-dev.txt', 'w') as file:\n",
    "    for item in std_dev:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the r2_scores list\n",
    "r2_scores_transposed = [list(x) for x in zip(*r2_scores)]\n",
    "# Pair up the stations with their r2_scores and store them in a dictionary\n",
    "scores = dict(zip(aquifers_list, r2_scores_transposed))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort them by the value in r2_scores[0]\n",
    "scores_sorted = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1][0])}\n",
    "scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the r2_scores\n",
    "joblib.dump(scores_sorted, '../reports/n-hits/n-hits-ground-water-r2-stations.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
