{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chronos test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/marcopeix/time-series-analysis/master/data/medium_views_published_holidays.csv')\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_scaling(x):\n",
    "    mean = np.mean(np.abs(x))\n",
    "\n",
    "    return x/mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['y'].values\n",
    "y_scaled = mean_scaling(y)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=1, nrows=2)\n",
    "\n",
    "ax1.plot(df['ds'][:200], y[:200], color='blue', label='Original')\n",
    "ax1.set_ylabel('Daily visits')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(df['ds'][:200], y_scaled[:200], color='orange', label='Scaled')\n",
    "ax2.set_ylabel('Daily visits (scaled)')\n",
    "ax2.legend()\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.qcut(y_scaled, q=100, labels=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=1, nrows=2)\n",
    "\n",
    "ax1.plot(df['ds'][:100], y_scaled[:100], color='blue', label='Scaled')\n",
    "ax1.set_ylabel('Daily visits (scaled)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.bar(df['ds'][:100], bins[:100], color='orange', label='Quantized')\n",
    "ax2.set_ylabel('Percentile')\n",
    "ax2.legend()\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with Chronos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U git+https://github.com/amazon-science/chronos-forecasting.git\n",
    "#%pip install neuralforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasetsforecast.m3 import M3\n",
    "from utilsforecast.losses import *\n",
    "from utilsforecast.evaluation import evaluate\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "Y_df, *_ = M3.load(directory='./', group='Monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-tiny\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "horizon = 12\n",
    "batch_size = 12\n",
    "\n",
    "\n",
    "actual = []\n",
    "chronos_tiny_preds = []\n",
    "\n",
    "start = time.time()\n",
    "all_timeseries = [\n",
    "    torch.tensor(sub_df[\"y\"].values[:-horizon])\n",
    "    for _, sub_df in Y_df.groupby(\"unique_id\")\n",
    "]\n",
    "\n",
    "for i in tqdm(range(0, len(all_timeseries), batch_size)):\n",
    "    batch_context = all_timeseries[i : i + batch_size]\n",
    "    forecast = pipeline.predict(batch_context, horizon)\n",
    "    predictions = np.quantile(forecast.numpy(), 0.5, axis=1)\n",
    "    chronos_tiny_preds.append(predictions)\n",
    "\n",
    "chronos_tiny_preds = np.concatenate(chronos_tiny_preds)\n",
    "chronos_tiny_duration = time.time() - start\n",
    "print(chronos_tiny_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-large\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "horizon = 12\n",
    "batch_size = 12\n",
    "\n",
    "\n",
    "actual = []\n",
    "chronos_large_preds = []\n",
    "\n",
    "start = time.time()\n",
    "all_timeseries = [\n",
    "    torch.tensor(sub_df[\"y\"].values[:-horizon])\n",
    "    for _, sub_df in Y_df.groupby(\"unique_id\")\n",
    "]\n",
    "for i in tqdm(range(0, len(all_timeseries), batch_size)):\n",
    "    batch_context = all_timeseries[i : i + batch_size]\n",
    "    forecast = pipeline.predict(batch_context, horizon)\n",
    "    predictions = np.quantile(forecast.numpy(), 0.5, axis=1)\n",
    "\n",
    "    chronos_large_preds.append(predictions)\n",
    "\n",
    "chronos_large_preds = np.concatenate(chronos_large_preds)\n",
    "chronos_large_duration = time.time() - start\n",
    "print(chronos_large_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = {\"unique_id\": [], \"chronos_tiny_pred\": [], \"chronos_large_pred\": []}\n",
    "for (name, _), tiny_pred, large_pred in zip(\n",
    "    Y_df.groupby(\"unique_id\"), chronos_tiny_preds, chronos_large_preds\n",
    "):\n",
    "    rows[\"unique_id\"].extend([name] * horizon)\n",
    "    rows[\"chronos_tiny_pred\"].extend(tiny_pred)\n",
    "    rows[\"chronos_large_pred\"].extend(large_pred)\n",
    "chronos_pred_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing N-BEATS and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.models import MLP, NBEATS\n",
    "from neuralforecast.losses.pytorch import HuberLoss\n",
    "from neuralforecast.core import NeuralForecast\n",
    "\n",
    "horizon = 12\n",
    "val_size = 12\n",
    "test_size = 12\n",
    "\n",
    "mlp = MLP(h=horizon, input_size=3*horizon, loss=HuberLoss(), devices=1, accelerator='cpu')\n",
    "\n",
    "start = time.time()\n",
    "nf = NeuralForecast(models=[mlp], freq='M')\n",
    "mlp_forecasts_df = nf.cross_validation(df=Y_df, val_size=val_size, test_size=test_size, n_windows=None, verbose=True)\n",
    "\n",
    "mlp_duration = time.time() - start\n",
    "\n",
    "mlp_forecasts_df.head()\n",
    "\n",
    "print(mlp_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbeats = NBEATS(h=horizon, input_size=3*horizon, loss=HuberLoss(), devices=1, accelerator='cpu')\n",
    "\n",
    "start = time.time()\n",
    "nf = NeuralForecast(models=[nbeats], freq='M')\n",
    "nbeats_forecasts_df = nf.cross_validation(df=Y_df, val_size=val_size, test_size=test_size, n_windows=None, verbose=True)\n",
    "\n",
    "nbeats_duration = time.time() - start\n",
    "\n",
    "nbeats_forecasts_df.head()\n",
    "\n",
    "print(nbeats_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install utilsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilsforecast.losses import mae, smape\n",
    "from utilsforecast.evaluation import evaluate\n",
    "\n",
    "\n",
    "forecast_df = mlp_forecasts_df\n",
    "forecast_df[\"NBEATS\"] = nbeats_forecasts_df[\"NBEATS\"]\n",
    "forecast_df = forecast_df.reset_index()\n",
    "forecast_df[\"Chronos-Tiny\"] = chronos_pred_df[\"chronos_tiny_pred\"]\n",
    "forecast_df[\"Chronos-Large\"] = chronos_pred_df[\"chronos_large_pred\"]\n",
    "\n",
    "evaluation = evaluate(\n",
    "    forecast_df,\n",
    "    metrics=[mae, smape],\n",
    "    models=[\"MLP\", \"NBEATS\", \"Chronos-Tiny\", \"Chronos-Large\"],\n",
    "    target_col=\"y\",\n",
    ")\n",
    "\n",
    "time = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\"Time\"],\n",
    "        \"MLP\": [mlp_duration],\n",
    "        \"NBEATS\": [nbeats_duration],\n",
    "        \"Chronos-Tiny\": [chronos_tiny_duration],\n",
    "        \"Chronos-Large\": [chronos_large_duration],\n",
    "    }\n",
    ")\n",
    "avg_metrics = (\n",
    "    evaluation.drop(columns=\"unique_id\").groupby(\"metric\").mean().reset_index()\n",
    ")\n",
    "avg_metrics = pd.concat([avg_metrics, time]).reset_index(drop=True)\n",
    "\n",
    "avg_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos-nf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
