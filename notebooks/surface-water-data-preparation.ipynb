{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import joblib\n",
    "\n",
    "# Geographical data plotting\n",
    "import folium\n",
    "from folium.map import Popup\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from geopy.distance import distance\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('../data/external/data.db')\n",
    "\n",
    "# Get a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# List all of the tables\n",
    "tables = cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "print(\"Tables in the database:\", table_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from each table into a pandas DataFrame\n",
    "dataframes = {table: pd.read_sql_query(f\"SELECT * FROM {table}\", conn) for table in table_names}\n",
    "\n",
    "# Name the datasets that we are going to work with\n",
    "watercourse_measurements = dataframes[table_names[2]]\n",
    "watercourse_stations = dataframes[table_names[1]]\n",
    "weather_locations = dataframes[table_names[6]]\n",
    "weather = dataframes[table_names[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watercourse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only leave data from 2010 to 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime\n",
    "watercourse_measurements['date'] = pd.to_datetime(watercourse_measurements['date'])\n",
    "\n",
    "# Filter data for years 2010 to 2017\n",
    "watercourse_measurements = watercourse_measurements[watercourse_measurements['date'].dt.year.between(2010, 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watercourse_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with station_id's as keys\n",
    "watercourse_measurements_stations = {station: data for station, data in watercourse_measurements.groupby('station_id')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watercourse_measurements_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep data that has rows in every year from 2010 to 2017\n",
    "keys_to_delete = []\n",
    "for key, data in watercourse_measurements_stations.items():\n",
    "    if data['date'].dt.year.nunique() < 8:\n",
    "        keys_to_delete.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the elements\n",
    "for key in keys_to_delete:\n",
    "    del watercourse_measurements_stations[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no dates missing\n",
    "for key, data in watercourse_measurements_stations.items():\n",
    "    # Define the start and end date\n",
    "    start_date = '2010-01-01'\n",
    "    end_date = '2017-12-31'\n",
    "    # Define the range\n",
    "    all_dates = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Reindex the DataFrame\n",
    "    data.set_index('date', inplace=True)\n",
    "    data = data.reindex(all_dates, fill_value=pd.NA).reset_index()\n",
    "    data.rename(columns={'index': 'date'}, inplace=True)\n",
    "    watercourse_measurements_stations[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the stations with more than 100 missing values\n",
    "keys_to_delete = []\n",
    "for key, data in watercourse_measurements_stations.items():\n",
    "    if (data['level'].isna().sum() > 100):\n",
    "        keys_to_delete.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete from the dictionary\n",
    "for key in keys_to_delete:\n",
    "    del watercourse_measurements_stations[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in watercourse_measurements_stations.items():\n",
    "    print(f\"{key}: {data['level'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values by interpolating the data\n",
    "for key, data in watercourse_measurements_stations.items():\n",
    "    # Linear interpolation of data\n",
    "    data['level'] = data['level'].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values at the beggining and at the end with backward and forward fill\n",
    "for key, data in watercourse_measurements_stations.items():\n",
    "    if (pd.isna(data['level']).any() == True):\n",
    "        watercourse_measurements_stations[key]['level'] = watercourse_measurements_stations[key]['level'].ffill().bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adition of column with relative differences of water level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in watercourse_measurements_stations.items():\n",
    "    data['level_diff'] = data['level'].diff().bfill()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a joblib file\n",
    "joblib.dump(watercourse_measurements_stations, '../data/interim/watercourse_by_stations.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' column from Unix timestamp to datetime\n",
    "weather['time'] = pd.to_datetime(weather['time'], unit='s')\n",
    "\n",
    "# Group by location_id and count the number of unique years\n",
    "location_years_counts = weather.groupby('location_id')['time'].apply(lambda x: x.dt.year.nunique())\n",
    "\n",
    "# Filter location_ids that have instances in all years from 2010 to 2017\n",
    "location_ids_all_years = location_years_counts[location_years_counts == 8].index.tolist()\n",
    "\n",
    "# Filter weather to include only location with instances in all years from 2010 to 2017\n",
    "weather = weather[weather['location_id'].isin(location_ids_all_years)]\n",
    "\n",
    "# Fetch all of the location_id's\n",
    "location_ids = weather['location_id'].unique()\n",
    "\n",
    "# Filter the locations that are in the final locations\n",
    "weather_locations = weather_locations[weather_locations['id'].isin(location_ids_all_years)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'time' column to 'date'\n",
    "weather = weather.rename(columns={'time': 'date'})\n",
    "\n",
    "# Remove the hour from date\n",
    "weather['date'] = weather['date'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns with most missing values\n",
    "weather = weather.drop('sun_duration', axis=1)\n",
    "weather = weather.drop('snow_depth', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataframe based on stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with dataframes for specific weather locations\n",
    "weather_by_locations = {location: data for location, data in weather.groupby('location_id')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the missing dates\n",
    "for key, data in weather_by_locations.items():\n",
    "    # Define the start and end date\n",
    "    start_date = '2010-01-01'\n",
    "    end_date = '2017-12-31'\n",
    "    # Define the range\n",
    "    all_dates = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Reindex the DataFrame\n",
    "    data.set_index('date', inplace=True)\n",
    "    data = data.reindex(all_dates, fill_value=pd.NA).reset_index()\n",
    "    data.rename(columns={'index': 'date'}, inplace=True)\n",
    "    weather_by_locations[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Check how many missing values there are for each column for each station\n",
    "# Delete the columns that have more than 100 missing values in any of the stations\n",
    "\n",
    "columns_to_delete = set() #To store the columns that we are going to delete\n",
    "\n",
    "# Let's first add the pressure and dew_point columns to the set.\n",
    "# In the article they found out, that they are either too correlated to other features\n",
    "# or uncorrelated to the target class\n",
    "columns_to_delete.update(['dew_point_min', 'dew_point_max', 'dew_point_avg', 'pressure_min', 'pressure_max', 'pressure_avg'])\n",
    "\n",
    "# Iterate over every station and every column\n",
    "for key, dataset in weather_by_locations.items():\n",
    "   for column, data in dataset.items():\n",
    "      missing_values = data.isna().sum()\n",
    "      # If there are more than 100 missing values,\n",
    "      # store the column name in a list\n",
    "      if missing_values > 100:\n",
    "         columns_to_delete.add(column)\n",
    "\n",
    "# Delete the columns from the list\n",
    "for key, dataset in weather_by_locations.items():\n",
    "   for column in columns_to_delete:\n",
    "      weather_by_locations[key] = weather_by_locations[key].drop(column, axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing values there are for each column for each station\n",
    "# Delete the stations that have more than 100 missing values in at least 1 column\n",
    "\n",
    "stations_to_delete = set() #To store the stations that we are going to delete\n",
    "columns_to_delete = set() # To store the columns that we are going to delete\n",
    "\n",
    "# Let's first add the pressure and dew_point columns to the set.\n",
    "# In the article they found out, that they are either too correlated to other features\n",
    "# or uncorrelated to the target class\n",
    "columns_to_delete.update(['dew_point_min', 'dew_point_max', 'dew_point_avg', 'pressure_min', 'pressure_max', 'pressure_avg'])\n",
    "\n",
    "# Iterate over every station and every column\n",
    "for key, dataset in weather_by_locations.items():\n",
    "   for column, data in dataset.items():\n",
    "      missing_values = data.isna().sum()\n",
    "      # If there are more than 100 missing values,\n",
    "      # store the station in a list\n",
    "      if missing_values > 100:\n",
    "        stations_to_delete.add(key)\n",
    "\n",
    "# Delete the columns from the list\n",
    "for key, dataset in weather_by_locations.items():\n",
    "   for column in columns_to_delete:\n",
    "      weather_by_locations[key] = weather_by_locations[key].drop(column, axis=1)\n",
    "\n",
    "# Delete the stations from the list\n",
    "for key in stations_to_delete:\n",
    "   del weather_by_locations[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the remaining columns interpolate, backward fill and forward fill the data\n",
    "\n",
    "for key, data in weather_by_locations.items():\n",
    "    for column in data.columns:\n",
    "        data[column] = data[column].interpolate(methon='linear')\n",
    "        data[column] = data[column].ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In weather_locations keep only the ones that are remaining after all of the filtering\n",
    "locations_to_keep = weather_by_locations.keys()\n",
    "weather_locations = weather_locations[weather_locations['id'].isin(locations_to_keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join ground water and weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watercourse_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the roman numbers\n",
    "watercourse_stations['name'] = watercourse_stations['name'].str.replace(r'\\b(I|II|III|IV)\\b', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove number 11\n",
    "watercourse_stations['name'] = watercourse_stations['name'].str.replace(r'\\b(11 )\\b', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the spaces at the beggining and at the end\n",
    "watercourse_stations['name'] = watercourse_stations['name'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stations that we are not going to use\n",
    "ids_to_keep = watercourse_measurements_stations.keys()\n",
    "\n",
    "watercourse_stations = watercourse_stations[watercourse_stations['id'].isin(ids_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize geocoder\n",
    "geolocator = Nominatim(user_agent=\"geocoder_for_slovenia\")\n",
    "\n",
    "# Function to geocode a place name with retries\n",
    "def geocode_place(place, retries=3, delay=1):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            location = geolocator.geocode(place)\n",
    "            time.sleep(delay) #nominatim supports only 1 query/second\n",
    "            if location:\n",
    "                return location.latitude, location.longitude\n",
    "            else:\n",
    "                return None, None\n",
    "        except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "            print(f\"Error geocoding {place}: {e}, retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    return None, None\n",
    "# Geocode each station name\n",
    "watercourse_stations['latitude'], watercourse_stations['longitude'] = zip(*watercourse_stations['name'].apply(geocode_place))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the watercourse stations and weather stations on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a map centered on Slovenia\n",
    "slovenia_map = folium.Map(location=[46.151241, 14.995463], zoom_start=8)\n",
    "\n",
    "# Add weather locations to the map (red colour)\n",
    "for _, row in weather_locations.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lng']],\n",
    "        popup=row['id'],\n",
    "        icon=folium.Icon(color='red')\n",
    "    ).add_to(slovenia_map)\n",
    "\n",
    "# Add aquifer stations to the map (blue colour)\n",
    "for _, row in watercourse_stations.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=row['id'],\n",
    "        icon=folium.Icon(color='blue')\n",
    "    ).add_to(slovenia_map)\n",
    "\n",
    "\n",
    "# Save the map to an HTML file\n",
    "slovenia_map.save('../data/interim/slovenia_map_surface_water.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the waeather and ground water data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest weather stations to the aquifer stations\n",
    "\n",
    "# Initialize dictionary to store closest locations\n",
    "closest_locations = {}\n",
    "\n",
    "# Iterate through each location in aquifer_stations\n",
    "for idx1, row1 in watercourse_stations.iterrows():\n",
    "    closest_location = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    # Iterate through each location in weather_locations\n",
    "    for idx2, row2 in weather_locations.iterrows():\n",
    "        # Calculate distance using geopy\n",
    "        dist = distance((row1['latitude'], row1['longitude']), (row2['lat'], row2['lng'])).km\n",
    "        \n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            closest_location = row2['id']\n",
    "    \n",
    "    # Store closest location_id in dictionary\n",
    "    closest_locations[row1['id']] = closest_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dictionaries\n",
    "\n",
    "\n",
    "# Initialize an empty dictionary to store combined dataframes\n",
    "combined_data = {}\n",
    "\n",
    "# Iterate through mapping_dict and merge corresponding dataframes based on 'date'\n",
    "for id1, loc_key in closest_locations.items():\n",
    "    if id1 in watercourse_measurements_stations and loc_key in weather_by_locations:\n",
    "        # For the aquifers we only choose the columns that are useful\n",
    "        df1 = watercourse_measurements_stations[id1][['date', 'station_id', 'level', 'level_diff']]\n",
    "        df2 = weather_by_locations[loc_key]\n",
    "        \n",
    "        # Cast both 'date' columns to datetime\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "        df2['date'] = pd.to_datetime(df2['date'])\n",
    "\n",
    "        # Merge dataframes based on 'date'\n",
    "        merged_df = pd.merge(df1, df2, on='date', how='outer')\n",
    "        \n",
    "        # Store merged dataframe in combined_data\n",
    "        combined_data[id1] = merged_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary without the generated features\n",
    "joblib.dump(combined_data, '../data/interim/surface-water-and-weather-no-new-features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_by_locations[639].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting the appropriate columns for 1 to days_to_shift days ahead\n",
    "days_to_shift=10\n",
    "\n",
    "\n",
    "# Specify the columns to shift\n",
    "columns_to_shift = ['level_diff', 'precipitation', 'snow_accumulation', 'temperature_avg',\n",
    "       'temperature_min', 'temperature_max', 'cloud_cover_avg',\n",
    "       'cloud_cover_min', 'cloud_cover_max', 'humidity_avg', 'humidity_min', 'humidity_max', \n",
    "       'precipitation_probability_avg', 'precipitation_probability_min', 'precipitation_probability_max',\n",
    "        'precipitation_intensity_avg', 'precipitation_intensity_min', 'precipitation_intensity_max']\n",
    "\n",
    "# Iterate over all of the dataframes in the dictionary\n",
    "for key, data in combined_data.items():\n",
    "    # Iterate over all of the columns in the columns_to_shift\n",
    "    for column in columns_to_shift:\n",
    "        # Iterate over all shifts\n",
    "        for shift in range (1, days_to_shift+1):\n",
    "            first_value = data[column].iloc[0]\n",
    "            data[f'{column}_shift{shift}'] = data[column].shift(shift)\n",
    "            # Fill the first values (NaN) with the first values from original columns\n",
    "            data[f'{column}_shift{shift}'] = data[f'{column}_shift{shift}'].fillna(first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new generated features into a list\n",
    "new_columns = combined_data[1060].columns.values[21:].tolist()\n",
    "\n",
    "# Combine the new columns with columns to shift\n",
    "# This is done so that original features and shifted features are averaged\n",
    "columns_to_average = columns_to_shift + new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the averages for all of the features (2 to 10 days)\n",
    "days_to_average = 10\n",
    "\n",
    "# Iterate over all of the dataframes in the dictionary\n",
    "for key, data in combined_data.items():\n",
    "    # Iterate over all of the columns in the columns_to_average\n",
    "    for column in columns_to_average:\n",
    "        # Iterate over all average window sizes\n",
    "        for average in range (2, days_to_average+1):\n",
    "            data[f'{column}_average{average}'] = data[column].rolling(window=average, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first days_to_average + days_to_shift days\n",
    "# When shifting and averaging, these days don't have the\n",
    "# appropriate values\n",
    "\n",
    "for key, data in combined_data.items():\n",
    "    combined_data[key] = combined_data[key][(days_to_average + days_to_shift):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dictionary\n",
    "joblib.dump(combined_data, '../data/interim/surface-water-and-weather.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[1060].columns.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
